{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.by import By\n",
    "import requests\n",
    "import time\n",
    "import random\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from tqdm import tqdm\n",
    "import json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_meta_data(doc_content):\n",
    "    meta_data = doc_content.find(\"table\", id=\"doc-meta\")\n",
    "    meta_data_dict = {}\n",
    "    for row in meta_data.find_all(\"tr\"):\n",
    "        header = row.find(\"th\").text.strip(\":\")\n",
    "        value = row.find(\"td\").text\n",
    "        meta_data_dict[header] = value\n",
    "\n",
    "    return meta_data_dict\n",
    "\n",
    "\n",
    "def extract_paragraphs(doc_content):\n",
    "    paragraphs = []\n",
    "    # Find all h2 headers\n",
    "    h2_headers = doc_content.find_all(\"h2\")\n",
    "\n",
    "    for header in h2_headers:\n",
    "        # Get the header text\n",
    "        header_text = header.text.strip()\n",
    "\n",
    "        # Find all paragraphs that follow this header until the next h2 or end\n",
    "        current = header.next_sibling\n",
    "        section_paragraphs = []\n",
    "\n",
    "        while current and not (current.name == \"h2\"):\n",
    "            if current.name == \"p\":\n",
    "                # Get paragraph text and check for <strong> tags\n",
    "                p_text = []\n",
    "                for content in current.contents:\n",
    "                    if content.name == \"strong\":\n",
    "                        break\n",
    "                    elif isinstance(content, str):\n",
    "                        p_text.append(content)\n",
    "                    else:\n",
    "                        p_text.append(content.text)\n",
    "\n",
    "                section_paragraphs.extend(\n",
    "                    [x for x in p_text if x.strip() and \"-------------\" not in x]\n",
    "                )\n",
    "            current = current.next_sibling\n",
    "\n",
    "        paragraphs.append({\"header\": header_text, \"paragraphs\": section_paragraphs})\n",
    "\n",
    "    return paragraphs\n",
    "\n",
    "\n",
    "def get_content(doc_content):\n",
    "    paragraphs = extract_paragraphs(doc_content)\n",
    "\n",
    "    return {\n",
    "        section[\"header\"].lower(): \"\\n\\n\".join(section[\"paragraphs\"])\n",
    "        for section in paragraphs\n",
    "    }\n",
    "\n",
    "\n",
    "def get_zveza(doc_content):\n",
    "    zveza = {}\n",
    "    for zveza_title in doc_content.find_all(\n",
    "        \"div\", class_=\"connection-category-wrapper\"\n",
    "    ):\n",
    "        zveza_detail = zveza_title.find_next_sibling(\"div\", class_=\"connection-detail1\")\n",
    "        zveza[zveza_title.text.strip()] = zveza_detail.text.strip()\n",
    "\n",
    "    if not zveza:\n",
    "        connection = doc_content.find(\"div\", id=\"doc-connection\")\n",
    "\n",
    "        if connection:\n",
    "            zveza[\"Without category\"] = connection.text.strip()\n",
    "\n",
    "    return zveza\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_table_links_from_page(page_number):\n",
    "    # Set up Chrome options\n",
    "    chrome_options = Options()\n",
    "    chrome_options.add_argument(\"--headless\")  # Run in headless mode\n",
    "\n",
    "    # Initialize the Chrome WebDriver\n",
    "    driver = webdriver.Chrome(options=chrome_options)\n",
    "\n",
    "    # Load the page\n",
    "    table_url = f\"https://www.sodnapraksa.si/?q=(podrocje:%22civilno%20procesno%20pravo%22)%20IN%20(oddelek:%22Civilni%20oddelek%22%20ALI%20oddelek:%22Gospodarski%20oddelek%22)%20IN%20(sodisce:%22Vrhovno%20sodi%C5%A1%C4%8De%22)&database[SOVS]=SOVS&_submit=i%C5%A1%C4%8Di&showType=table&order=date&direction=desc&page={page_number}&rowsPerPage=20\"\n",
    "    # table_url = f\"https://www.sodnapraksa.si/?q=(podrocje:%22kazensko%20procesno%20pravo%22)%20IN%20(oddelek:%22Kazenski%20oddelek%22)&database[SOVS]=SOVS&_submit=i%C5%A1%C4%8Di&showType=table&order=date&direction=desc&page={page_number}&rowsPerPage=20\"\n",
    "    driver.get(table_url)\n",
    "\n",
    "    # Wait for the table to load\n",
    "    wait = WebDriverWait(driver, 1)\n",
    "    _ = wait.until(EC.presence_of_element_located((By.ID, \"results-table\")))\n",
    "\n",
    "    # Get the page source after JavaScript has run\n",
    "    page_source = driver.page_source\n",
    "    soup = BeautifulSoup(page_source, \"html.parser\")\n",
    "\n",
    "    # Clean up\n",
    "    driver.quit()\n",
    "\n",
    "    table_content = soup.find(\"table\", id=\"results-table\")\n",
    "    links = table_content.find_all(\"a\", href=True)\n",
    "    hrefs = [\n",
    "        f\"https://www.sodnapraksa.si{link.get('href').replace(' ', '%20')}\"\n",
    "        for link in links\n",
    "    ]\n",
    "    return hrefs\n",
    "\n",
    "\n",
    "def extract_data_from_table_link(url):\n",
    "    try:\n",
    "        # Add random delay between 0-500ms\n",
    "        time.sleep(random.randint(0, 1000) / 1000)\n",
    "\n",
    "        response = requests.get(url)\n",
    "        soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "        doc_content = soup.find(\"div\", id=\"doc-content\")\n",
    "\n",
    "        meta = extract_meta_data(doc_content)\n",
    "        content = get_content(doc_content)\n",
    "        zveza = get_zveza(doc_content)\n",
    "\n",
    "        meta[\"url\"] = url\n",
    "        return {\"meta\": meta, \"content\": content, \"zveza\": zveza}\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting data from {url}: {e}\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 623/623 [1:16:13<00:00,  7.34s/it]\n"
     ]
    }
   ],
   "source": [
    "SUBFOLDER = \"cpp_in__co_ali_go__in_vs\"\n",
    "for page_num in tqdm(range(0 + 193, 816)):\n",
    "    links = get_table_links_from_page(page_num)\n",
    "\n",
    "    with ThreadPoolExecutor(max_workers=3) as executor:\n",
    "        results = list(executor.map(extract_data_from_table_link, links))\n",
    "\n",
    "    for result in results:\n",
    "        if result:\n",
    "            evidencna = result[\"meta\"][\"Evidenčna številka\"]\n",
    "            filename = f\"./data/scraped/{SUBFOLDER}/{evidencna}.json\"\n",
    "\n",
    "            with open(filename, \"w\", encoding=\"utf-8\") as f:\n",
    "                json.dump(result, f, indent=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# url = \"https://www.sodnapraksa.si/?q=VS2006070&database[SOVS]=SOVS&_submit=i%C5%A1%C4%8Di&rowsPerPage=20&page=0&id=2012032113046733\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract_data_from_table_link(url)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vs-poc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
